{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OI8SaifSRrG"
      },
      "source": [
        "# General properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEONSdMGqixY"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "LANG_EST = \"est_Latn\"\n",
        "LANG_VEP = \"vep_Latn\" # Veps\n",
        "LANG_MOK = \"mdf_Cyrl\" # Moksha\n",
        "LANG_OLO = \"olo_Latn\" # Livvi Karelian\n",
        "LANG_MHR = \"mhr_Cyrl\" # Meadow Mari\n",
        "LANG_RUS = \"rus_Cyrl\"\n",
        "\n",
        "BT_TAG = \"Â¶\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "\n",
        "RANDOM_SEED = 42 # Used only for back-translated data selection\n",
        "TAG_SYNTHETIC_SRC = False\n",
        "PARALLEL_CAP = None # Limit to how much parallel data can be included\n",
        "SYNTHETIC_RATIO = 2 # How much synthetic data to use (back-translation)\n",
        "FOLDER_NAME = \"training\"\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 5\n",
        "\n",
        "CHECKPOINT = \"facebook/nllb-200-distilled-600M\""
      ],
      "metadata": {
        "id": "B-DoIinS51W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSU2HLpmSQxu"
      },
      "outputs": [],
      "source": [
        "from transformers import NllbTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = NllbTokenizer.from_pretrained(CHECKPOINT)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24pY_ehMlrT"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXFiTrT1Mpsx"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrLnJiuBMsRN"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "  with open(path, \"r\") as file:\n",
        "    sents = file.read().splitlines()\n",
        "\n",
        "  return sents\n",
        "\n",
        "def prepare_parallel_dataset(source, target, /, nr_examples = None, tag_enabled = False, tag_token = None):\n",
        "  if tag_enabled and tag_token is None:\n",
        "    raise ValueError(\"If tag_enabled is True then tag_token can not be None\")\n",
        "\n",
        "  source_path, source_lang = source[\"path\"], source[\"lang\"]\n",
        "  target_path, target_lang = target[\"path\"], target[\"lang\"]\n",
        "\n",
        "  source_sents = load_data(source_path)\n",
        "  target_sents = load_data(target_path)\n",
        "\n",
        "  if PARALLEL_CAP is not None:\n",
        "    nr_examples = PARALLEL_CAP\n",
        "\n",
        "  if nr_examples is not None:\n",
        "      combined = list(zip(source_sents, target_sents))\n",
        "\n",
        "      rng = random.Random(RANDOM_SEED)\n",
        "      rng.shuffle(combined)\n",
        "\n",
        "      selection = combined[:nr_examples]\n",
        "      source_sents, target_sents = zip(*selection)\n",
        "\n",
        "  if tag_enabled:\n",
        "    source_sents = [tag_token + sent for sent in source_sents]\n",
        "\n",
        "  translations = []\n",
        "  for src_sent, tgt_sent in zip(source_sents, target_sents):\n",
        "    example = {\n",
        "        source_lang: src_sent,\n",
        "        target_lang: tgt_sent,\n",
        "    }\n",
        "    translations.append(example)\n",
        "\n",
        "  data = {\n",
        "      \"id\": [i + 1 for i in range(len(translations))],\n",
        "      \"translations\": translations,\n",
        "  }\n",
        "\n",
        "  return Dataset.from_dict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amDgpFyqQlpj"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "# EST-VEP\n",
        "est_vep = {\n",
        "    \"path\": \"vep/et-vep.et.normal\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "vep_est = {\n",
        "    \"path\": \"vep/et-vep.vep.normal\",\n",
        "    \"lang\": LANG_VEP,\n",
        "}\n",
        "\n",
        "# EST-MOK\n",
        "est_mdf = {\n",
        "    \"path\": \"mdf/et-mdf.et.normal\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "mdf_est = {\n",
        "    \"path\": \"mdf/et-mdf.mdf.normal\",\n",
        "    \"lang\": LANG_MOK,\n",
        "}\n",
        "\n",
        "# EST-OLO\n",
        "est_olo = {\n",
        "    \"path\": \"olo/et-olo.et.normal\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "olo_est = {\n",
        "    \"path\": \"olo/et-olo.olo.normal\",\n",
        "    \"lang\": LANG_OLO,\n",
        "}\n",
        "\n",
        "# EST-MHR\n",
        "est_mhr = {\n",
        "    \"path\": \"mhr/et-mhr.et.normal\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "mhr_est = {\n",
        "    \"path\": \"mhr/et-mhr.mhr.normal\",\n",
        "    \"lang\": LANG_MHR,\n",
        "}\n",
        "\n",
        "# Preparing datasets\n",
        "parallel_est_vep = prepare_parallel_dataset(est_vep, vep_est)\n",
        "parallel_est_mdf = prepare_parallel_dataset(est_mdf, mdf_est)\n",
        "parallel_est_olo = prepare_parallel_dataset(est_olo, olo_est)\n",
        "parallel_est_mhr = prepare_parallel_dataset(est_mhr, mhr_est)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeIG-V2wLNXw"
      },
      "outputs": [],
      "source": [
        "# Preparing back-translated data\n",
        "\n",
        "est_vep_examples = len(parallel_est_vep) * SYNTHETIC_RATIO\n",
        "est_mdf_examples = len(parallel_est_mdf) * SYNTHETIC_RATIO\n",
        "est_olo_examples = len(parallel_est_olo) * SYNTHETIC_RATIO\n",
        "est_mhr_examples = len(parallel_est_mhr) * SYNTHETIC_RATIO\n",
        "\n",
        "print(\"est-vep examples:\", est_vep_examples)\n",
        "print(\"est-mdf examples:\", est_mdf_examples)\n",
        "print(\"est-olo examples:\", est_olo_examples)\n",
        "print(\"est-mhr examples:\", est_mhr_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gpBDgIjML4K"
      },
      "outputs": [],
      "source": [
        "# The back-translated data source refers to the language that was used to generate the translations and\n",
        "# target the language that the source was translated to.\n",
        "# When we use this data in training we will switch the translation direction because it has been shown\n",
        "# that using the synthetic sentence in the source side is better.\n",
        "\n",
        "# Config\n",
        "\n",
        "# EST-VEP\n",
        "est_vep_src = {\n",
        "    \"path\": \"vep/train.est_Latn-vep_Latn.est_Latn\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "est_vep_tgt = {\n",
        "    \"path\": \"vep/train.est_Latn-vep_Latn.vep_Latn\",\n",
        "    \"lang\": LANG_VEP,\n",
        "}\n",
        "\n",
        "vep_est_src = {\n",
        "    \"path\": \"vep/train.vep_Latn-est_Latn.vep_Latn\",\n",
        "    \"lang\": LANG_VEP,\n",
        "}\n",
        "\n",
        "vep_est_tgt = {\n",
        "    \"path\": \"vep/train.vep_Latn-est_Latn.est_Latn\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "# EST-MOK\n",
        "est_mdf_src = {\n",
        "    \"path\": \"mdf/train.est_Latn-mdf_Cyrl.est_Latn\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "est_mdf_tgt = {\n",
        "    \"path\": \"mdf/train.est_Latn-mdf_Cyrl.mdf_Cyrl\",\n",
        "    \"lang\": LANG_MOK,\n",
        "}\n",
        "\n",
        "mdf_est_src = {\n",
        "    \"path\": \"mdf/train.mdf_Cyrl-est_Latn.mdf_Cyrl\",\n",
        "    \"lang\": LANG_MOK,\n",
        "}\n",
        "\n",
        "mdf_est_tgt = {\n",
        "    \"path\": \"mdf/train.mdf_Cyrl-est_Latn.est_Latn\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "# EST-OLO\n",
        "est_olo_src = {\n",
        "    \"path\": \"olo/train.est_Latn-olo_Latn.est_Latn\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "est_olo_tgt = {\n",
        "    \"path\": \"olo/train.est_Latn-olo_Latn.olo_Latn\",\n",
        "    \"lang\": LANG_OLO,\n",
        "}\n",
        "\n",
        "olo_est_src = {\n",
        "    \"path\": \"olo/train.olo_Latn-est_Latn.olo_Latn\",\n",
        "    \"lang\": LANG_OLO,\n",
        "}\n",
        "\n",
        "olo_est_tgt = {\n",
        "    \"path\": \"olo/train.olo_Latn-est_Latn.est_Latn\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "# EST-MHR\n",
        "est_mhr_src = {\n",
        "    \"path\": \"mhr/train.est_Latn-mhr_Cyrl.est_Latn\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "est_mhr_tgt = {\n",
        "    \"path\": \"mhr/train.est_Latn-mhr_Cyrl.mhr_Cyrl\",\n",
        "    \"lang\": LANG_MHR,\n",
        "}\n",
        "\n",
        "mhr_est_src = {\n",
        "    \"path\": \"mhr/train.mhr_Cyrl-est_Latn.mhr_Cyrl\",\n",
        "    \"lang\": LANG_MHR,\n",
        "}\n",
        "\n",
        "mhr_est_tgt = {\n",
        "    \"path\": \"mhr/train.mhr_Cyrl-est_Latn.est_Latn\",\n",
        "    \"lang\": LANG_EST,\n",
        "}\n",
        "\n",
        "# Directions are switched on purpose for reason mentioned above\n",
        "translated_dataset_vep_est = prepare_parallel_dataset(est_vep_tgt, est_vep_src, nr_examples = est_vep_examples, tag_enabled = False, tag_token = BT_TAG)\n",
        "translated_dataset_est_vep = prepare_parallel_dataset(vep_est_tgt, vep_est_src, nr_examples = est_vep_examples, tag_enabled = False, tag_token = BT_TAG)\n",
        "\n",
        "translated_dataset_mdf_est = prepare_parallel_dataset(est_mdf_tgt, est_mdf_src, nr_examples = est_mdf_examples, tag_enabled = False, tag_token = BT_TAG)\n",
        "translated_dataset_est_mdf = prepare_parallel_dataset(mdf_est_tgt, mdf_est_src, nr_examples = est_mdf_examples, tag_enabled = False, tag_token = BT_TAG)\n",
        "\n",
        "translated_dataset_olo_est = prepare_parallel_dataset(est_olo_tgt, est_olo_src, nr_examples = est_olo_examples, tag_enabled = False, tag_token = BT_TAG)\n",
        "translated_dataset_est_olo = prepare_parallel_dataset(olo_est_tgt, olo_est_src, nr_examples = est_olo_examples, tag_enabled = False, tag_token = BT_TAG)\n",
        "\n",
        "translated_dataset_mhr_est = prepare_parallel_dataset(est_mhr_tgt, est_mhr_src, nr_examples = est_mhr_examples, tag_enabled = False, tag_token = BT_TAG)\n",
        "translated_dataset_est_mhr = prepare_parallel_dataset(mhr_est_tgt, mhr_est_src, nr_examples = est_mhr_examples, tag_enabled = False, tag_token = BT_TAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iLOK7kKRv-P"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4xeQvMyR2X8"
      },
      "source": [
        "## Add new language to the tokenizer\n",
        "Source: [Fine tuning NLLB-200](https://cointegrated.medium.com/how-to-fine-tune-a-nllb-200-model-for-translating-a-new-language-a37fc706b865)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXGFC8lXoXdj"
      },
      "source": [
        "### Add language token to tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE0_utmuWrQQ"
      },
      "outputs": [],
      "source": [
        "def add_token_to_tokenizer(lang_token, similar_token):\n",
        "  old_len = len(tokenizer) - int(lang_token in tokenizer.added_tokens_encoder)\n",
        "  tokenizer.lang_code_to_id[lang_token] = old_len - 1\n",
        "  tokenizer.id_to_lang_code[old_len - 1] = lang_token\n",
        "  # always move \"mask\" to the last position\n",
        "  tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n",
        "\n",
        "  tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n",
        "  tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n",
        "\n",
        "  if lang_token not in tokenizer._additional_special_tokens:\n",
        "      tokenizer._additional_special_tokens.append(lang_token)\n",
        "  # clear the added lang_token encoder; otherwise a new lang_token may end up there by mistake\n",
        "  tokenizer.added_tokens_encoder = {}\n",
        "  tokenizer.added_tokens_decoder = {}\n",
        "\n",
        "  added_token_id = tokenizer.convert_tokens_to_ids(lang_token)\n",
        "  similar_lang_id = tokenizer.convert_tokens_to_ids(similar_token)\n",
        "\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "  # moving the embedding for \"mask\" to its new position\n",
        "  model.model.shared.weight.data[added_token_id + 1] = model.model.shared.weight.data[added_token_id]\n",
        "  # initializing new language token with a token of a similar language\n",
        "  model.model.shared.weight.data[added_token_id] = model.model.shared.weight.data[similar_lang_id]\n",
        "\n",
        "new_tokens = [\n",
        "    (LANG_VEP, LANG_EST),\n",
        "    (LANG_OLO, LANG_EST),\n",
        "    (LANG_MOK, LANG_RUS),\n",
        "    (LANG_MHR, LANG_RUS),\n",
        "]\n",
        "\n",
        "for new, similar in new_tokens:\n",
        "  add_token_to_tokenizer(new, similar)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure that everything is there\n",
        "\n",
        "for new, _ in new_tokens:\n",
        "  print(f\"{new}:\", tokenizer.convert_tokens_to_ids(new))"
      ],
      "metadata": {
        "id": "wvZnkYM7gi8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH_VODe5owKn"
      },
      "source": [
        "## Tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI9q2pul7xrM"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "\n",
        "def preprocess(examples, src_lang, tgt_lang):\n",
        "  tokenizer.src_lang = src_lang\n",
        "  tokenizer.tgt_lang = tgt_lang\n",
        "  sources = [example[src_lang] for example in examples[\"translations\"]]\n",
        "  targets = [example[tgt_lang] for example in examples[\"translations\"]]\n",
        "  inputs = tokenizer(sources, text_target=targets, max_length=200, truncation=True)\n",
        "  return inputs\n",
        "\n",
        "test_size = 0.1\n",
        "\n",
        "# EST-VEP\n",
        "split_est_vep_par = parallel_est_vep.train_test_split(test_size=test_size)\n",
        "split_est_vep = translated_dataset_vep_est.train_test_split(test_size=test_size)\n",
        "split_vep_est = translated_dataset_est_vep.train_test_split(test_size=test_size)\n",
        "\n",
        "est_vep_train = [\n",
        "  split_est_vep_par[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_VEP}),\n",
        "  split_est_vep_par[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_VEP, \"tgt_lang\": LANG_EST}),\n",
        "  split_est_vep[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_VEP}),\n",
        "  split_vep_est[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_VEP, \"tgt_lang\": LANG_EST}),\n",
        "]\n",
        "\n",
        "est_vep_test = [\n",
        "  split_est_vep_par[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_VEP}),\n",
        "  split_est_vep_par[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_VEP, \"tgt_lang\": LANG_EST}),\n",
        "  split_est_vep[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_VEP}),\n",
        "  split_vep_est[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_VEP, \"tgt_lang\": LANG_EST}),\n",
        "]\n",
        "\n",
        "# EST-MOK\n",
        "split_est_mdf_par = parallel_est_mdf.train_test_split(test_size=test_size)\n",
        "split_est_mdf = translated_dataset_mdf_est.train_test_split(test_size=test_size)\n",
        "split_mdf_est = translated_dataset_est_mdf.train_test_split(test_size=test_size)\n",
        "\n",
        "est_mdf_train = [\n",
        "  split_est_mdf_par[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_MOK}),\n",
        "  split_est_mdf_par[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_MOK, \"tgt_lang\": LANG_EST}),\n",
        "  split_est_mdf[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_MOK}),\n",
        "  split_mdf_est[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_MOK, \"tgt_lang\": LANG_EST}),\n",
        "]\n",
        "\n",
        "est_mdf_test = [\n",
        "  split_est_mdf_par[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_MOK}),\n",
        "  split_est_mdf_par[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_MOK, \"tgt_lang\": LANG_EST}),\n",
        "  split_est_mdf[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_MOK}),\n",
        "  split_mdf_est[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_MOK, \"tgt_lang\": LANG_EST}),\n",
        "]\n",
        "\n",
        "# EST-OLO\n",
        "split_est_olo_par = parallel_est_olo.train_test_split(test_size=test_size)\n",
        "split_est_olo = translated_dataset_olo_est.train_test_split(test_size=test_size)\n",
        "split_olo_est = translated_dataset_est_olo.train_test_split(test_size=test_size)\n",
        "\n",
        "est_olo_train = [\n",
        "  split_est_olo_par[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_OLO}),\n",
        "  split_est_olo_par[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_OLO, \"tgt_lang\": LANG_EST}),\n",
        "  split_est_olo[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_OLO}),\n",
        "  split_olo_est[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_OLO, \"tgt_lang\": LANG_EST}),\n",
        "]\n",
        "\n",
        "est_olo_test = [\n",
        "  split_est_olo_par[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_OLO}),\n",
        "  split_est_olo_par[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_OLO, \"tgt_lang\": LANG_EST}),\n",
        "  split_est_olo[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_OLO}),\n",
        "  split_olo_est[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_OLO, \"tgt_lang\": LANG_EST}),\n",
        "]\n",
        "\n",
        "\n",
        "# EST-MHR\n",
        "split_est_mhr_par = parallel_est_mhr.train_test_split(test_size=test_size)\n",
        "split_est_mhr = translated_dataset_mhr_est.train_test_split(test_size=test_size)\n",
        "split_mhr_est = translated_dataset_est_mhr.train_test_split(test_size=test_size)\n",
        "\n",
        "est_mhr_train = [\n",
        "  split_est_mhr_par[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_MHR}),\n",
        "  split_est_mhr_par[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_MHR, \"tgt_lang\": LANG_EST}),\n",
        "  split_est_mhr[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_MHR}),\n",
        "  split_mhr_est[\"train\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_MHR, \"tgt_lang\": LANG_EST}),\n",
        "]\n",
        "\n",
        "est_mhr_test = [\n",
        "  split_est_mhr_par[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_MHR}),\n",
        "  split_est_mhr_par[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_MHR, \"tgt_lang\": LANG_EST}),\n",
        "  split_est_mhr[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_EST, \"tgt_lang\": LANG_MHR}),\n",
        "  split_mhr_est[\"test\"].map(preprocess, batched=True, fn_kwargs={\"src_lang\": LANG_MHR, \"tgt_lang\": LANG_EST}),\n",
        "]\n",
        "\n",
        "# Creating combined train and test datasets\n",
        "\n",
        "train_datasets = [\n",
        "  *est_vep_train,\n",
        "  *est_mdf_train,\n",
        "  *est_olo_train,\n",
        "  *est_mhr_train,\n",
        "]\n",
        "\n",
        "test_datasets = [\n",
        "  *est_vep_test,\n",
        "  *est_mdf_test,\n",
        "  *est_olo_test,\n",
        "  *est_mhr_test,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XwFENEb8tww"
      },
      "outputs": [],
      "source": [
        "def prepare_tokenized_ds(dataset):\n",
        "  dataset = dataset.remove_columns(['id', 'translations'])\n",
        "  dataset.set_format('torch')\n",
        "  dataset.column_names\n",
        "\n",
        "  return dataset\n",
        "\n",
        "processed_train_datasets = []\n",
        "processed_test_datasets = []\n",
        "\n",
        "for dataset in train_datasets:\n",
        "  dataset = prepare_tokenized_ds(dataset)\n",
        "  processed_train_datasets.append(dataset)\n",
        "\n",
        "for dataset in test_datasets:\n",
        "  dataset = prepare_tokenized_ds(dataset)\n",
        "  processed_test_datasets.append(dataset)\n",
        "\n",
        "tokenized_train = concatenate_datasets(processed_train_datasets)\n",
        "tokenized_test = concatenate_datasets(processed_test_datasets)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=CHECKPOINT, padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3WzeAlIroQ5"
      },
      "source": [
        "## Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efYMwHH3-Bgo"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyvWkeJX-Eso"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    tokenized_train, shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_test, shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcidGgbV-I8f"
      },
      "outputs": [],
      "source": [
        "for batch in train_dataloader:\n",
        "  print({k: v.shape for k, v in batch.items()})\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eqTr2Ra_6Kk"
      },
      "outputs": [],
      "source": [
        "# Test: can model accept inputs\n",
        "outputs = model(**batch)\n",
        "print(outputs.loss, outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df9mMJLpr1PS"
      },
      "source": [
        "# Fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npYTjGn1Adn6"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THgrzKUKAp_u"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = EPOCHS\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHQpPGHKBEMC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps97cIcwBH4F"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "def train(model, dataloader, optimizer, scheduler):\n",
        "  total_loss = 0\n",
        "  model.train()\n",
        "  for batch in dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    progress_bar.update(1)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def test(model, dataloader):\n",
        "  total_loss = 0\n",
        "  total_bleu = 0\n",
        "  total_chr = 0\n",
        "  model.eval()\n",
        "  for batch in dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    predictions = tokenizer.batch_decode(predictions.tolist(), skip_special_tokens=True)\n",
        "    labels = [[id if id >= 0 else 1 for id in sent] for sent in batch[\"labels\"].tolist()]\n",
        "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    total_loss += outputs.loss.item()\n",
        "    total_bleu += sacrebleu.corpus_bleu(predictions, [labels]).score\n",
        "    total_chr += sacrebleu.corpus_chrf(predictions, [labels], word_order = 2).score\n",
        "\n",
        "  return total_loss / len(dataloader), total_bleu / len(dataloader), total_chr / len(dataloader)\n",
        "\n",
        "train_loss_log, dev_loss_log, dev_bleu_log, dev_chr_log, lr_log = [], [], [], [], []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_dataloader, optimizer, lr_scheduler)\n",
        "    dev_loss, dev_bleu, dev_chr = test(model, test_dataloader)\n",
        "    train_loss_log.append(train_loss)\n",
        "    dev_loss_log.append(dev_loss)\n",
        "    dev_bleu_log.append(dev_bleu)\n",
        "    dev_chr_log.append(dev_chr)\n",
        "    lr_log.append(lr_scheduler.get_lr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQfXPSJIYvEG"
      },
      "outputs": [],
      "source": [
        "print(train_loss_log)\n",
        "print(dev_loss_log)\n",
        "print(dev_bleu_log)\n",
        "print(dev_chr_log)\n",
        "print(lr_log)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the metrics so that they can be plotted later\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "FOLDER_NAME = \"trainings\"\n",
        "\n",
        "now = int(time.time())\n",
        "\n",
        "path = Path('.') / f\"{FOLDER_NAME}-{now}\"\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def store_metric(items, name, base_path):\n",
        "  metric_path = base_path / name\n",
        "  with metric_path.open(\"w\") as f:\n",
        "    for metric in items:\n",
        "      f.write(f\"{metric}\\n\")\n",
        "\n",
        "\n",
        "metrics = [\n",
        "  (train_loss_log, \"train_loss\"),\n",
        "  (dev_loss_log, \"dev_loss\"),\n",
        "  (dev_bleu_log, \"bleu\"),\n",
        "  (dev_chr_log, \"chr\"),\n",
        "  (lr_log, \"lr\"),\n",
        "]\n",
        "\n",
        "for items, name in metrics:\n",
        "  store_metric(items, name, path)\n"
      ],
      "metadata": {
        "id": "UEFiTe1m7tcd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}